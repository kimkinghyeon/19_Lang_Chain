{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable \n",
    "* Runnable 은 런타임에 실행될 수 있는 모든 객체를 의미한다.\n",
    "\n",
    "### Runnable 종류\n",
    "* invoke : 입력에 대해 체인을 호출한다.\n",
    "* stream : 응답의 청크를 스트리밍한다.\n",
    "* batch : 입력 목록에 대한 일괄 처리를 수행한다.\n",
    "\n",
    "### 이외에 비동기 메소드도 존재한다.\n",
    "* astream : 비동기적으로 응답의 청크를 비동기 스트리밍한다.\n",
    "* ainvoke : 비동기적으로 입력에 대해 체인을 호출한다.\n",
    "* abatch : 비동기적으로 입력 목록에 대해 일괄 처리를 수행한다.\n",
    "* astream_log : 최종 응답 및 발생하는 중간단계를 스트리밍한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path='../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name = 'gpt-4o-mini',\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{lecture}에 대해 3문장으로 설명해줘\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"lecture\":\"임진왜란\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stream [실시간츨력]\n",
    "\n",
    "* 데이터 스트림을 생성하고, 스트림을 반복하여 각 데이터의 내용을 즉시 출력한다.\n",
    "* end=\"\" 인자를 사용해 줄바꿈을 하지 않을 수 있다.\n",
    "* flush=True 인자를 사용해 즉시 출력 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in chain.stream({\"lecture\":\"고종황제\"}):\n",
    "    print(token,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch [단위실행]\n",
    "* 여러개의 딕셔너리를 포함하는 리스트를 인자로 받아, 각각의 입력에 대한 체인을 실행한다.\n",
    "* max_cocurrency : 최대 병렬 처리 수리르 지정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.batch(\n",
    "    [\n",
    "        {\"lecture\":\"이순신\"},\n",
    "        {\"lecture\":\"조선선조\"},\n",
    "        {\"lecture\":\"임진왜란\"},       \n",
    "    ],\n",
    "    config={\"max_concurrency\":3}\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Parallel (병렬성)\n",
    "* LCEL를 사용하여 체인을 구성할때, 여러 체인을 동시에 실행 할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = (\n",
    "    PromptTemplate.from_template(\"{country} 의 수도는 어디야?\")\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2 = (\n",
    "    PromptTemplate.from_template(\"{country} 의 수장은 누구야?\")\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# 병렬 실행 체인\n",
    "combined = RunnableParallel(capital= chain1, area=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain1.invoke({\"country\":\"프로이센\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain2.invoke({\"country\":\"독일제국\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = combined.invoke({'country':'독일제국'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"capital\"])\n",
    "print(result[\"area\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnablePassThrough\n",
    "* 입력을 그대로 전달하는 역할\n",
    "* 단독으로 호출될 경우 단순히 입력을 받아 그대로 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{num}의 약수를 알려줘\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10의 약수는 1, 2, 5, 10입니다.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke()를 통해 실행할때는 입력이 딕셔너리여야 하지만,\n",
    "# 1개의 변수만 템플릿에 작성이 되었다면, 값만 전달해도 된다.\n",
    "chain.invoke(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10의 약수는 1, 2, 5, 10입니다.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "passthrough_chain = {'num': RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "passthrough_chain.invoke({'num':10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** RunnablePassthrough.assign()**\n",
    "* 입력값으로 들어온 값의 key/value 쌍을 새롭게 할당된 key/value 쌍을 합쳐준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 1, 'new_date': 3}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnablePassthrough.assign(new_date=lambda x:x['num']*3).invoke({'num':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnbaleLambda\n",
    "* 입력값에 대한 추가 처리를 수행할 수 있다.\n",
    "* 람다 함수를 사용해 입력 값에 대한 추가 처리를 진행\n",
    "* 날씨, 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_today(num):\n",
    "    # print(\"num : \", num)\n",
    "    return datetime.now().strftime(\"%b-%d\")\n",
    "\n",
    "# get_today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"{today}가 생일인 유명인 {n}명을 알려줘\"\n",
    ")\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "# chain 생성\n",
    "\n",
    "chain = (\n",
    "    {\"today\": RunnableLambda(get_today), \"n\": itemgetter(\"n\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"n\":3}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
